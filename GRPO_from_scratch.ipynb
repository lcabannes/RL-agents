{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80517dbc",
      "metadata": {
        "id": "80517dbc"
      },
      "source": [
        "# GRPO Training project: teach an LLM to do additions, again"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DAEbh3jBadc7",
      "metadata": {
        "id": "DAEbh3jBadc7"
      },
      "source": [
        "In this notebook, you'll find:\n",
        "* A basic Transformer with basic tokenizer\n",
        "* A basic dataset for additions\n",
        "* A classical pre-trainer, minimizing cross entropy loss\n",
        "* A Vanilla GRPO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MB8lj855LgHl",
      "metadata": {
        "id": "MB8lj855LgHl"
      },
      "source": [
        "You're not supposed to edit the existing code (you can if you want to...).\n",
        "You should implement one (or more) of the following:\n",
        "* GRPO with PPO (the `usual` one)\n",
        "* RLOO\n",
        "* ReMax\n",
        "* DPO\n",
        "* RAFT\n",
        "* your own RLHF method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ae993bb9",
      "metadata": {
        "id": "ae993bb9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "OzGh9ahKF17h",
      "metadata": {
        "id": "OzGh9ahKF17h"
      },
      "outputs": [],
      "source": [
        "num_digits = 3\n",
        "\n",
        "dataset_size = 64_000\n",
        "train_proportion = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fabd151a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fabd151a",
        "outputId": "e7b36965-8f38-4c49-cf03-0109ab723eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c054bed",
      "metadata": {
        "id": "6c054bed"
      },
      "source": [
        "## Step 1: Construct a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "t6aC9uNeIR6C",
      "metadata": {
        "id": "t6aC9uNeIR6C"
      },
      "outputs": [],
      "source": [
        "pad_token=\"[PAD]\"\n",
        "eos_token=\"[EOS]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "g2QiF-otFur3",
      "metadata": {
        "id": "g2QiF-otFur3"
      },
      "outputs": [],
      "source": [
        "class character_level_tokenizer:\n",
        "    \"\"\"\n",
        "    character-level\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
        "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
        "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
        "        self.ntokens = len(self.vocab)\n",
        "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
        "\n",
        "    def clean(self, text):\n",
        "        \"\"\"\n",
        "        removes all characters not in the vocabulary\n",
        "        \"\"\"\n",
        "        out = re.sub(self.pattern, \"\", text)\n",
        "        return out\n",
        "\n",
        "    def pre_tokenization(self, text):\n",
        "        \"\"\"\n",
        "        character-level\n",
        "        \"\"\"\n",
        "        return [c for c in text]\n",
        "\n",
        "    def encode(self, text):\n",
        "        text_list = self.pre_tokenization(self.clean(text))\n",
        "        return [self.token_to_id[c] for c in text_list]\n",
        "\n",
        "    def decode(self, token_list):\n",
        "        return \"\".join([self.id_to_token[x] for x in token_list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "QuCc6jF5F8hK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuCc6jF5F8hK",
        "outputId": "e42dddaa-dacd-471e-b7d0-48f17004dd87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = character_level_tokenizer()\n",
        "ntokens = tokenizer.ntokens\n",
        "ntokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8FXW2K-1Jd-P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FXW2K-1Jd-P",
        "outputId": "fff6c129-8820-4b3c-8caa-b43710370098"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([1, 2, 10, 4, 2, 11], '12+42=')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"12 + 42 =\"\n",
        "inputs = tokenizer.encode(prompt)\n",
        "inputs, tokenizer.decode(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "491af297",
      "metadata": {
        "id": "491af297"
      },
      "source": [
        "## Step 2: Create a dataset for arithmetic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "daa90f31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa90f31",
        "outputId": "c23f0b74-6948-43a9-9e6f-5846f4b325b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('988+241=', '1229')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sample_datapoint(num_digits = 3):\n",
        "    a_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    b_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
        "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
        "    a_str = \"\".join([str(x) for x in a_list])\n",
        "    b_str = \"\".join([str(x) for x in b_list])\n",
        "    sum_int = a_int + b_int\n",
        "    return (a_str + \"+\" + b_str + \"=\", str(sum_int))\n",
        "\n",
        "sample_datapoint(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b6e861d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e861d2",
        "outputId": "7b639165-7809-4f2d-a067-f21547c293b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('275+418=', '693'),\n",
              " ('885+534=', '1419'),\n",
              " ('641+638=', '1279'),\n",
              " ('674+086=', '760')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = []\n",
        "for _ in range(dataset_size):\n",
        "    data.append(sample_datapoint(num_digits))\n",
        "data[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fee85050",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fee85050",
        "outputId": "69def39c-084c-4074-f296-908925038748"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(57600, 6400)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = data[: int(train_proportion * dataset_size)]\n",
        "data_test = data[int(train_proportion * dataset_size):]\n",
        "\n",
        "len(data_train),len(data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37200598",
      "metadata": {
        "id": "37200598"
      },
      "source": [
        "## Step 3: Construct a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "91674239",
      "metadata": {
        "id": "91674239"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4eb278ab",
      "metadata": {
        "id": "4eb278ab"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Transformer):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__(d_model=ninp,\n",
        "                                               nhead=nhead,\n",
        "                                               dim_feedforward=nhid,\n",
        "                                               num_encoder_layers=nlayers)\n",
        "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
        "\n",
        "    def forward(self, src):\n",
        "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "        self.src_mask = mask\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output_enc = self.encoder(src, mask=self.src_mask)\n",
        "        output_dec = self.decoder(output_enc)\n",
        "        return output_dec, output_enc # I think we should remove the logsoftmax here because we do it again later\n",
        "        # return F.log_softmax(output_dec, dim=-1), output_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1d568cc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d568cc4",
        "outputId": "7677a7ea-d06a-46a8-85f3-44274cc008a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/loic/projects/llm_mva/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
              "  (input_emb): Embedding(14, 128)\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = TransformerModel(ntoken = ntokens,\n",
        "                         ninp = 128,\n",
        "                         nhead = 16,\n",
        "                         nhid = 64,\n",
        "                         nlayers = 8)\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a6PmJSo95N4C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6PmJSo95N4C",
        "outputId": "f0177342-43b6-46c6-b079-aab40db326d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 668942\n"
          ]
        }
      ],
      "source": [
        "print(\"number of parameters: {}\".format(sum([x.numel() for x in model.parameters()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e35d113",
      "metadata": {
        "id": "2e35d113"
      },
      "source": [
        "### Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8f2f06e0",
      "metadata": {
        "id": "8f2f06e0"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompts, new_tokens = 5, mode = \"greedy\", num_samples = 1, temperature = 0.8):\n",
        "    input_tensor = torch.repeat_interleave(prompts, repeats = num_samples, dim = 1).to(device)\n",
        "    # (prompt_length, batch_size * num_samples)\n",
        "    for _ in range(new_tokens):\n",
        "        output, _ = model(input_tensor) # (prompt_length, batch_size * num_samples, ntokens)\n",
        "        logits = output[-1,:,:] # (batch_size * num_samples, ntokens)\n",
        "        if mode == \"greedy\":\n",
        "            tokens = torch.argmax(logits, -1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        else: # mode == \"sampling\"\n",
        "            logits /= temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            tokens = torch.multinomial(probs, num_samples = 1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        input_tensor = torch.cat((input_tensor, tokens), 0)\n",
        "    return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d76d1b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d76d1b19",
        "outputId": "7c084e50-8269-4f24-859a-4f95f8bd143d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 2, 10,  3, 11,  6, 13, 13, 13, 13]], device='cuda:0'),\n",
              " '2+3=6[EOS][EOS][EOS][EOS]')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"2+3=\"\n",
        "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "output = generate(model, prompt_tensor).view((1,-1))\n",
        "output, tokenizer.decode(output[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "00954ddc",
      "metadata": {
        "id": "00954ddc"
      },
      "outputs": [],
      "source": [
        "def pad(token_list, type_list = \"prompts\"):\n",
        "    max_length = max([len(x) for x in token_list])\n",
        "    out = []\n",
        "    for x in token_list:\n",
        "        if type_list == \"prompts\":\n",
        "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
        "        if type_list == \"answers\":\n",
        "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
        "    return out, max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2c84beab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c84beab",
        "outputId": "3e53a680-31e1-4cd6-8ce0-d96009def972"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['[PAD][PAD]1+1=', '21+35='], ['2[EOS][PAD]', '56[EOS]'])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
        "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
        "padded_prompts, _ = pad(prompts, \"prompts\")\n",
        "padded_answers, _ = pad(answers, \"answers\")\n",
        "padded_prompts, padded_answers\n",
        "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "264f9227",
      "metadata": {
        "id": "264f9227"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, i, batch_size):\n",
        "    data = data_train if split == 'train' else data_test\n",
        "\n",
        "    prompts = [data[i][0] for i in range(i, i + batch_size)]\n",
        "    encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]\n",
        "    padded_prompts, prompt_length = pad(encoded_prompts, \"prompts\")\n",
        "\n",
        "    answers = [data[i][1] for i in range(i, i + batch_size)]\n",
        "    encoded_answers = [tokenizer.encode(answer) for answer in answers]\n",
        "    padded_answers, answers_length = pad(encoded_answers, \"answers\")\n",
        "\n",
        "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
        "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
        "    return X, Y, prompt_length, answers_length, prompts, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "91e281ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e281ad",
        "outputId": "2a97779d-08e9-485a-9588-07e699dc6b77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 16]), torch.Size([5, 16]), 8, 4, '457+456=', '913')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, Y, prompt_length, answers_length, prompts, answers = get_batch(\"train\", 43, 16)\n",
        "X.shape, Y.shape, prompt_length, answers_length, prompts[0], answers[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "113e1fd1",
      "metadata": {
        "id": "113e1fd1"
      },
      "source": [
        "## Step 4: Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "KfmcSdPwp3K6",
      "metadata": {
        "id": "KfmcSdPwp3K6"
      },
      "outputs": [],
      "source": [
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1cfcd10a",
      "metadata": {
        "id": "1cfcd10a"
      },
      "outputs": [],
      "source": [
        "def evaluate(batch_size = batch_size):\n",
        "    # Turn on evaluation mode disables dropout.\n",
        "    model.eval()\n",
        "    correct = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"test\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
        "            output = generate(model, prompts, answers_length + 1) # (prompt_length + answers_length + 1, batch_size)\n",
        "            answers_tokens = output[prompt_length:, :] # (answers_length + 1, batch_size), contains tokens\n",
        "            equality_test = answers_tokens == target_answers # (answers_length + 1, batch_size), contains boolean values\n",
        "            correct += torch.all(equality_test, axis=0).float().sum()\n",
        "        accuracy = correct / len(data_test)\n",
        "    return accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ac335b05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac335b05",
        "outputId": "1355d497-45f0-440a-90b8-30a7f2818091"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c54061a",
      "metadata": {
        "id": "4c54061a"
      },
      "source": [
        "## Step 5: Train the model, classical approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b827e567",
      "metadata": {
        "id": "b827e567"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5b140ba3",
      "metadata": {
        "id": "5b140ba3"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "batch_size = 16\n",
        "learning_rate = 8e-4\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3638a75d",
      "metadata": {
        "id": "3638a75d"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.\n",
        "        start_time = time.time()\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
        "            input_tensor = torch.cat((prompts, target_answers), 0) # (prompt_length + answers_length + 1, batch_size)\n",
        "            model.zero_grad()\n",
        "            output, _ = model(input_tensor) # (prompt_length + answers_length + 1, batch_size, ntokens)\n",
        "            output_answers = output[prompt_length-1:-1,:,:].reshape(-1, ntokens) # ((answers_length + 1) * batch_size, ntokens)\n",
        "            target_answers = target_answers.view(-1)\n",
        "            loss = F.cross_entropy(output_answers, target_answers)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                cur_loss = total_loss / log_interval\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
        "                                                                                                            elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "                total_loss = 0\n",
        "                start_time = time.time()\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"arithmetic.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy\n",
        "        if test_accuracy > 0.5:\n",
        "            print(f\"reached high enough baseline for GRPO, breaking\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4e2a8490",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e2a8490",
        "outputId": "492562cc-d243-4314-ab56-d17d41c070ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  0.36 | loss  0.09 | perplexity     1.09\n",
            "|  1200/ 3600 batches | ms/batch  0.36 | loss  0.07 | perplexity     1.07\n",
            "|  1800/ 3600 batches | ms/batch  0.34 | loss  0.07 | perplexity     1.07\n",
            "|  2400/ 3600 batches | ms/batch  0.34 | loss  0.07 | perplexity     1.07\n",
            "|  3000/ 3600 batches | ms/batch  0.30 | loss  0.07 | perplexity     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 21.53s | test accuracy  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  0.31 | loss  0.06 | perplexity     1.07\n",
            "|  1200/ 3600 batches | ms/batch  0.30 | loss  0.06 | perplexity     1.07\n",
            "|  1800/ 3600 batches | ms/batch  0.40 | loss  0.06 | perplexity     1.06\n",
            "|  2400/ 3600 batches | ms/batch  0.39 | loss  0.06 | perplexity     1.06\n",
            "|  3000/ 3600 batches | ms/batch  0.33 | loss  0.06 | perplexity     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 22.10s | test accuracy  0.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  0.31 | loss  0.04 | perplexity     1.04\n",
            "|  1200/ 3600 batches | ms/batch  0.37 | loss  0.04 | perplexity     1.04\n",
            "|  1800/ 3600 batches | ms/batch  0.33 | loss  0.04 | perplexity     1.04\n",
            "|  2400/ 3600 batches | ms/batch  0.31 | loss  0.03 | perplexity     1.03\n",
            "|  3000/ 3600 batches | ms/batch  0.35 | loss  0.03 | perplexity     1.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 21.22s | test accuracy  0.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  0.38 | loss  0.04 | perplexity     1.04\n",
            "|  1200/ 3600 batches | ms/batch  0.33 | loss  0.03 | perplexity     1.03\n",
            "|  1800/ 3600 batches | ms/batch  0.31 | loss  0.03 | perplexity     1.03\n",
            "|  2400/ 3600 batches | ms/batch  0.32 | loss  0.02 | perplexity     1.02\n",
            "|  3000/ 3600 batches | ms/batch  0.38 | loss  0.02 | perplexity     1.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 22.47s | test accuracy  0.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "reached high enough baseline for GRPO, breaking\n"
          ]
        }
      ],
      "source": [
        "train()\n",
        "torch.save(model.state_dict(), \"trained_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "56d9d440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56d9d440",
        "outputId": "b876390d-4396-4a95-e600-e2230317dbec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "748+112=860[EOS]\t actual result: 860\n",
            "866+855=1721[EOS]\t actual result: 1721\n",
            "526+573=1109[EOS]\t actual result: 1099\n",
            "297+164=461[EOS]\t actual result: 461\n",
            "455+360=815[EOS]\t actual result: 815\n",
            "158+085=243[EOS]\t actual result: 243\n",
            "571+182=753[EOS]\t actual result: 753\n",
            "871+814=1685[EOS]\t actual result: 1685\n",
            "744+874=1618[EOS]\t actual result: 1618\n",
            "550+170=710[EOS]\t actual result: 720\n",
            "851+079=920[EOS]\t actual result: 930\n",
            "342+092=434[EOS]\t actual result: 434\n",
            "937+437=1374[EOS]\t actual result: 1374\n",
            "231+480=711[EOS]\t actual result: 711\n",
            "907+054=961[EOS]\t actual result: 961\n",
            "753+381=1135[EOS]\t actual result: 1134\n",
            "287+702=989[EOS]\t actual result: 989\n",
            "392+664=1056[EOS]\t actual result: 1056\n",
            "562+692=1254[EOS]\t actual result: 1254\n",
            "016+614=629[EOS]\t actual result: 630\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa4c591",
      "metadata": {
        "id": "cfa4c591"
      },
      "source": [
        "## Step 4 bis: Vanilla GRPO training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff83f72",
      "metadata": {
        "id": "aff83f72"
      },
      "source": [
        "### Custom reward functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3c548bf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c548bf7",
        "outputId": "b5d45345-22eb-445a-b888-6ad22e96d7b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 0.0)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def accuracy_reward(output, answer):\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return 1. if output == answer else 0.\n",
        "\n",
        "accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), accuracy_reward(\"123\", \"124\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e1f02762",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f02762",
        "outputId": "7271bdf4-4ecf-44ee-ec86-8c815ffaa9a5",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0, -0.008064516129032258)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def distance_accuracy_reward(output, answer):\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    int_output = int(output)\n",
        "    int_answer = int(answer)\n",
        "    return - abs(int_output - int_answer) / max(int_output, int_answer)\n",
        "\n",
        "distance_accuracy_reward(\"123[EOS]\", \"123\"), distance_accuracy_reward(\"123[PAD]\", \"124\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b42a0d70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b42a0d70",
        "outputId": "a79ca9d6-a79b-4640-c172-4db78e433256"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def digit_accuracy_reward(output, answer):\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return sum(c1 == c2 for (c1,c2) in zip(output, answer)) / max(len(output), len(answer))\n",
        "\n",
        "digit_accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), digit_accuracy_reward(\"123[EOS]\", \"123\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a41603b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a41603b2",
        "outputId": "33dbf431-4177-4ae3-d36c-08ff8a2261fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 1.0, 0.0)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def reward_format(output):\n",
        "    pattern = r\"\\d+\\[EOS\\](\\[PAD\\])*$\"\n",
        "    return 1. if bool(re.match(pattern, output)) else 0.\n",
        "\n",
        "reward_format(\"123[EOS][PAD][PAD]\"), reward_format(\"123[EOS]\"), reward_format(\"123\"),"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4482e411",
      "metadata": {
        "id": "4482e411"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "cf764cb0",
      "metadata": {
        "id": "cf764cb0"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "num_samples = 16\n",
        "temperature = .8\n",
        "mu = 2\n",
        "beta = 0.1\n",
        "eps = 0.2\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)\n",
        "\n",
        "reward_fun = digit_accuracy_reward\n",
        "reward_format = reward_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f15cdced",
      "metadata": {
        "id": "f15cdced"
      },
      "outputs": [],
      "source": [
        "def compute_rewards(text_outputs, answers):\n",
        "    repeated_answers = [answer for answer in answers for _ in range(num_samples)]\n",
        "    rewards = torch.tensor(\n",
        "        [0.2 * reward_format(output) + 0.8 * reward_fun(output, answer)\n",
        "         for output, answer in zip(text_outputs, repeated_answers)],\n",
        "        dtype=torch.float32,\n",
        "        device=device\n",
        "    )\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e22be0d4",
      "metadata": {
        "id": "e22be0d4"
      },
      "outputs": [],
      "source": [
        "def calculate_grpo_advantages(rewards):\n",
        "    # reshape rewards to group by prompt\n",
        "    # compute mean and standard deviation for each prompt group\n",
        "    mean_rewards = rewards.view(-1, num_samples).mean(dim=1)\n",
        "    std_rewards = rewards.view(-1, num_samples).std(dim=1)\n",
        "    # print(f\"mean rewards: {mean_rewards.mean()}\")\n",
        "    # expand the means and stds to match the original flat rewards tensor shape\n",
        "    mean_rewards = mean_rewards.repeat_interleave(num_samples, dim=0)\n",
        "    std_rewards = std_rewards.repeat_interleave(num_samples, dim=0)\n",
        "    # normalize rewards to get advantages\n",
        "    advantages = (rewards - mean_rewards) / (std_rewards + 1e-5)\n",
        "    return advantages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "faac5c99",
      "metadata": {
        "id": "faac5c99"
      },
      "outputs": [],
      "source": [
        "def compute_log_probs(model, outputs, prompt_length):\n",
        "    logits, _ = model(outputs)\n",
        "    # logits.shape = (prompt_length + answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "\n",
        "    # we only need the log probabilities for the new tokens\n",
        "    # this introduces a shift: the logits for a position are the predictions for the next token\n",
        "    logits = logits[prompt_length-1:-1, :, :]\n",
        "    # logits.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "\n",
        "    # convert raw logits into log probabilities along the vocabulary axis\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "    return log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2612214",
      "metadata": {
        "id": "b2612214"
      },
      "outputs": [],
      "source": [
        "def compute_loss(advantages, log_probs, responses):\n",
        "    # reshape responses from (answers_length + 1, batch_size * num_samples)\n",
        "    # to (answers_length + 1, batch_size * num_samples, 1) for gathering\n",
        "    responses = responses.unsqueeze(-1)\n",
        "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "    # responses.shape = (answers_length + 1, batch_size * num_samples)\n",
        "    # gather the log probability for each token in responses\n",
        "    selected_log_probs = log_probs.gather(dim=-1, index=responses)\n",
        "    # remove the extra last dimension to get back to shape (answers_length + 1, batch_size * num_samples).\n",
        "    selected_log_probs = selected_log_probs.squeeze(-1)\n",
        "\n",
        "    # normalize\n",
        "    selected_log_probs = (selected_log_probs - selected_log_probs.mean(-1, keepdim=True)) / (selected_log_probs.std(-1, keepdim=True) + 1e-5)\n",
        "\n",
        "    # advantages.shape = (batch_size * num_samples)\n",
        "    # we use the same advantages for all tokens in the response\n",
        "    loss = -(advantages.unsqueeze(dim=0) * selected_log_probs).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_ppo_loss(advantages, log_probs, old_log_probs, ref_log_probs, responses):\n",
        "    responses = responses.unsqueeze(-1)\n",
        "    selected_log_probs = log_probs.gather(dim=-1, index=responses).squeeze(-1)\n",
        "    old_selected_log_probs = old_log_probs.gather(dim=-1, index=responses).squeeze(-1)\n",
        "    ref_selected_log_probs = ref_log_probs.gather(dim=-1, index=responses).squeeze(-1)\n",
        "\n",
        "    ratios = torch.exp(selected_log_probs - old_selected_log_probs)\n",
        "    advantages = advantages.unsqueeze(dim=0)\n",
        "\n",
        "    surr1 = ratios * advantages\n",
        "    surr2 = torch.clamp(ratios, 1-eps, 1+eps) * advantages\n",
        "\n",
        "\n",
        "    clip_loss = - torch.min(surr1, surr2).mean()\n",
        "\n",
        "    KL =  torch.exp(ref_selected_log_probs - selected_log_probs) - (ref_selected_log_probs - selected_log_probs)  - 1 \n",
        "    KL = KL.mean()\n",
        "    return clip_loss + beta * KL\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "4f49e44d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "initial_model = model\n",
        "\n",
        "model = deepcopy(initial_model)\n",
        "model.load_state_dict(torch.load(\"trained_model.pt\"))\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd03205",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch 28.41\n",
            "|  1200/ 3600 batches | ms/batch 56.92\n",
            "|  1800/ 3600 batches | ms/batch 85.84\n",
            "|  2400/ 3600 batches | ms/batch 114.63\n",
            "|  3000/ 3600 batches | ms/batch 143.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 173.80s | test accuracy  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch 27.09\n",
            "|  1200/ 3600 batches | ms/batch 54.10\n",
            "|  1800/ 3600 batches | ms/batch 81.22\n",
            "|  2400/ 3600 batches | ms/batch 108.28\n",
            "|  3000/ 3600 batches | ms/batch 135.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 165.55s | test accuracy  0.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch 27.61\n",
            "|  1200/ 3600 batches | ms/batch 54.98\n",
            "|  1800/ 3600 batches | ms/batch 82.36\n",
            "|  2400/ 3600 batches | ms/batch 110.90\n",
            "|  3000/ 3600 batches | ms/batch 138.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 167.92s | test accuracy  0.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch 27.09\n",
            "|  1200/ 3600 batches | ms/batch 54.06\n",
            "|  1800/ 3600 batches | ms/batch 81.04\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[107], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m prompts \u001b[38;5;241m=\u001b[39m prompts\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (prompt_length, batch_size)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# generate samples for each prompt\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnew_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manswers_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# outputs.shape = (prompt_length + answers_length + 1, batch_size * num_samples)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[prompt_length:, i]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     33\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))]\n",
            "Cell \u001b[0;32mIn[15], line 12\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, prompts, new_tokens, mode, num_samples, temperature)\u001b[0m\n\u001b[1;32m     10\u001b[0m         logits \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m temperature\n\u001b[1;32m     11\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# (1, batch_size * num_samples)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((input_tensor, tokens), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_tensor\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "best_test_accuracy = None\n",
        "test_accuracy = evaluate()\n",
        "print('-' * 89)\n",
        "print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "print('-' * 89)\n",
        "\n",
        "# switch eval for train model (enables dropout)\n",
        "model.train()\n",
        "ref_model = deepcopy(model) # reference model for KL divergence penalty\n",
        "old_model = deepcopy(model) # old model for PPO ratio\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    ref_model.load_state_dict(model.state_dict()) # update ref model every epoch like in the GRPO paper\n",
        "    epoch_start_time = time.time()\n",
        "    start_time = time.time()\n",
        "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "\n",
        "        # get a batch of prompts and answers\n",
        "        prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\n",
        "        prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "\n",
        "        # generate samples for each prompt\n",
        "        outputs = generate(model,\n",
        "                            prompts,\n",
        "                            new_tokens=answers_length + 1,\n",
        "                            mode=\"sampling\",\n",
        "                            num_samples=num_samples,\n",
        "                            temperature=temperature)\n",
        "        # outputs.shape = (prompt_length + answers_length + 1, batch_size * num_samples)\n",
        "        text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
        "                        for i in range(outputs.size(1))]\n",
        "        \n",
        "        old_model.load_state_dict(model.state_dict()) # update the old model before the update steps like in the paper\n",
        "\n",
        "        # compute old log probabilities for ratio and ref for KL divergence penalty\n",
        "        with torch.no_grad():\n",
        "            ref_log_probs = compute_log_probs(ref_model, outputs, prompt_length).detach()\n",
        "            old_log_probs = compute_log_probs(old_model, outputs, prompt_length).detach()\n",
        "\n",
        "        # compute rewards\n",
        "        rewards = compute_rewards(text_outputs, answers)\n",
        "\n",
        "        # compute advantages\n",
        "        advantages = calculate_grpo_advantages(rewards)\n",
        "\n",
        "        # compute loss\n",
        "        responses = outputs[prompt_length:, :]\n",
        "\n",
        "        for inner_iter in range(mu): # do mu iteration on the generated trajectories like in the paper\n",
        "\n",
        "            new_log_probs = compute_log_probs(model, outputs, prompt_length)\n",
        "            loss = compute_ppo_loss(advantages, new_log_probs, old_log_probs, ref_log_probs, responses)\n",
        "\n",
        "            # optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "\n",
        "        if i % log_interval == 0 and batch > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| {:5d}/{:5d} batches | ms/batch {:5.2f}'.format(batch, len(data_train) // batch_size, elapsed))\n",
        "\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "    print('-' * 89)\n",
        "    # Save the model if the test accuracy is the best we've seen so far.\n",
        "    if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "        with open(\"arithmetic_vanilla_GRPO.pt\", 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_test_accuracy = test_accuracy\n",
        "    if test_accuracy > 0.99:\n",
        "        print(f\"achieved near perfect accuracy after {epoch} epochs\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824ca075",
      "metadata": {
        "id": "824ca075"
      },
      "outputs": [],
      "source": [
        "def train_vanilla_GRPO(verbose = False):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "\n",
        "    # switch eval for train model (enables dropout)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        start_time = time.time()\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "\n",
        "            # get a batch of prompts and answers\n",
        "            prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "\n",
        "            # generate samples for each prompt\n",
        "            outputs = generate(model,\n",
        "                               prompts,\n",
        "                               new_tokens = answers_length + 1,\n",
        "                               mode = \"sampling\",\n",
        "                               num_samples = num_samples,\n",
        "                               temperature = temperature)\n",
        "            # outputs.shape = (prompt_length + answers_length + 1, batch_size * num_samples)\n",
        "            text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
        "                            for i in range(outputs.size(1))]\n",
        "\n",
        "            # compute rewards\n",
        "            rewards = compute_rewards(text_outputs, answers)\n",
        "\n",
        "            # compute advantages\n",
        "            advantages = calculate_grpo_advantages(rewards)\n",
        "\n",
        "            # compute log probabilities\n",
        "            log_probs = compute_log_probs(model, outputs, prompt_length)\n",
        "\n",
        "            # compute loss\n",
        "            responses = outputs[prompt_length:, :]\n",
        "            loss = compute_loss(advantages, log_probs, responses)\n",
        "\n",
        "            # optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f}'.format(batch, len(data_train) // batch_size, elapsed))\n",
        "                if verbose:\n",
        "                    print(\"\\nquestion:\", questions[0],\n",
        "                      \"\\nanswer\", answers[0],\n",
        "                      \"\\noutput:\", text_outputs[:num_samples],\n",
        "                      \"\\nreward:\", rewards[:num_samples],\n",
        "                      \"\\nadvantage:\", advantages[:num_samples], \"\\n\")\n",
        "\n",
        "                start_time = time.time()\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"arithmetic_vanilla_GRPO.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b02716ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "b02716ff",
        "outputId": "d3ae59c3-1d5f-4e40-e428-097a04b4edb1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_vanilla_GRPO(verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeVn935w5BSp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeVn935w5BSp",
        "outputId": "2014648b-68e6-4af6-ce9c-b28a3d580c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "812+903=1715[EOS]\t actual result: 1715\n",
            "719+186=905[EOS]\t actual result: 905\n",
            "989+423=1412[EOS]\t actual result: 1412\n",
            "647+953=1600[EOS]\t actual result: 1600\n",
            "896+601=1497[EOS]\t actual result: 1497\n",
            "375+255=630[EOS]\t actual result: 630\n",
            "286+746=1032[EOS]\t actual result: 1032\n",
            "833+872=1705[EOS]\t actual result: 1705\n",
            "209+711=920[EOS]\t actual result: 920\n",
            "061+971=1032[EOS]\t actual result: 1032\n",
            "868+804=1672[EOS]\t actual result: 1672\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "607+682=1289[EOS]\t actual result: 1289\n",
            "004+041=45[EOS]\t actual result: 45\n",
            "215+238=453[EOS]\t actual result: 453\n",
            "804+016=820[EOS]\t actual result: 820\n",
            "132+505=637[EOS]\t actual result: 637\n",
            "037+028=65[EOS]\t actual result: 65\n",
            "348+632=980[EOS]\t actual result: 980\n",
            "363+104=467[EOS]\t actual result: 467\n",
            "272+611=883[EOS]\t actual result: 883\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
